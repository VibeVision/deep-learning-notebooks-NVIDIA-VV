{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Hosting the Model\n",
    "\n",
    "In this notebook, you'll learn strategies to optimize Triton Server to improve the performance of your deployment.\n",
    "\n",
    "\n",
    "**[2.1 Concurrent Model Execution](#2.1-Concurrent-Model-Execution)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.1 Exercise: Usage Considerations](#2.1.1-Exercise:-Usage-Considerations)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.2 Implementation](#2.1.2-Implementation)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.3 Exercise: Configure Multiple Instance Groups](#2.1.3-Exercise:-Configure-Multiple-Instance-Groups)<br>\n",
    "**[2.2 Scheduling Strategies](#2.2-Scheduling-Strategies)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.1 Stateless Inference](#2.2.1-Stateless-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.2 Stateful Inference](#2.2.2-Stateful-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.3 Pipelines / Ensembles](#2.2.3-Pipelines-/-Ensembles)<br>\n",
    "**[2.3 Dynamic Batching](#2.3-Dynamic-Batching)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.3.1 Exercise: Implement Dynamic Batching](#2.3.1-Exercise:-Implement-Dynamic-Batching)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've executed customer requests sequentially, in the order they have arrived at the server, and used a static batch of size 8 for any requests to our server. This has not only left our GPUs heavily underutilized, but has also significantly affected the latency of responses received from the server. This is not an uncommon situation. Unless you are developing an application that processes large volumes of data in batch, you will likely be sending individual inference requests from the user application, leading to even further underutilization. As we have seen in the previous notebook, model optimizations do help considerably to accelerate model execution.  However, they do not change the fact that when serving is implemented naively, the nature of the inference workload leads to GPU underutilization.\n",
    "\n",
    "Inference servers, such as NVIDIA Triton, implement a wide range of features that allow us to improve the GPU utilization and improve request latency. The three that we will discuss in this class are:<br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/architecture.html#section-concurrent-model-execution\">Concurrent model execution</a></br>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/models_and_schedulers.html\">Scheduling</a> <br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/model_configuration.html#section-dynamic-batcher\">Dynamic batching</a> <br/>\n",
    "\n",
    "\n",
    "Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/quickstart.html\">Triton documentation</a> and its <a href=\"https://github.com/NVIDIA/triton-inference-server\">source code</a> for further information about the mechanisms and configurations that can help improve model inference performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Concurrent Model Execution\n",
    "The Triton architecture allows multiple models and/or multiple instances of the same model to execute in parallel on a single GPU. The following figure shows an example with two models: `model0` and `model1`. Assuming Triton is not currently processing any request, when two requests arrive simultaneously, one for each model, Triton immediately schedules both of them onto the GPU, and the GPUâ€™s hardware scheduler begins working on both computations in parallel. </br>\n",
    "\n",
    "<img src=\"images/multi_model_exec.png\"/><br/>\n",
    "\n",
    "#### Default Behavior\n",
    "\n",
    "By default, if multiple requests for the same model arrive at the same time, Triton will serialize their execution by scheduling only one at a time on the GPU, as shown in the following figure.\n",
    "\n",
    "<img src=\"images/multi_model_serial_exec.png\"/><br/>\n",
    "\n",
    "Triton provides an instance-group feature that allows each model to specify how many parallel executions of that model should be allowed. Each such enabled parallel execution is referred to as an *execution instance*. By default, Triton gives each model a single execution instance, which means that only a single execution of the model is allowed to be in progress at a time as shown in the above figure. \n",
    "\n",
    "#### Instance Groups\n",
    "By using the *instance-group* setting, the number of execution instances for a model can be increased. The following figure shows model execution when `model1` is configured to allow three execution instances. As shown in the figure, the first three `model1` inference requests are immediately executed in parallel on the GPU. The fourth `model1` inference request must wait until one of the first three executions completes before beginning.\n",
    "\n",
    "<img src=\"images/multi_model_parallel_exec.png\"/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Exercise: Usage Considerations\n",
    "\n",
    "For most models, the Triton feature that provides the largest performance improvement is *dynamic batching*. The key advantages of dynamic batching over setting up multiple instance execution are:\n",
    "- No overhead for model parameter storage\n",
    "- No overhead related to model parameter fetch from the GPU memory\n",
    "- Better utilization of the GPU resources\n",
    "\n",
    "Before we look at the configuration for multiple model execution, let's execute our model again using a single instance, and observe the resource utilization of the GPU. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Steps\n",
    "1. Launch a terminal window from the JupyterLab launch page.  If you need to open a new launch page, click the '+' icon on the left sidebar menu. You can then use a drag-and-drop action to move the terminal to a sub-window configuration  for better viewing.\n",
    "2. Execute the following command in the terminal before you run the performance tool:<br>\n",
    "\n",
    "```\n",
    "watch -n0.5 nvidia-smi\n",
    "```\n",
    "    You should see an output that resembles:\n",
    "<img src=\"images/NVIDIASMI.png\" style=\"position:relative; left:30px;\" width=800/>\n",
    "\n",
    "3. Execute the same benchmark we used in the previous notebook, but with the batch size reduced to 1, and observe the <code>nvidia-smi</code> output again.  Pay special attention to the memory consumption and GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previous configuration.\n",
    "modelVersion=\"1\"\n",
    "precision=\"fp32\"\n",
    "batchSize=\"1\"\n",