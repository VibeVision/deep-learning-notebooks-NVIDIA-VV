{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Hosting the Model\n",
    "\n",
    "In this notebook, you'll learn strategies to optimize Triton Server to improve the performance of your deployment.\n",
    "\n",
    "\n",
    "**[2.1 Concurrent Model Execution](#2.1-Concurrent-Model-Execution)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.1 Exercise: Usage Considerations](#2.1.1-Exercise:-Usage-Considerations)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.2 Implementation](#2.1.2-Implementation)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.3 Exercise: Configure Multiple Instance Groups](#2.1.3-Exercise:-Configure-Multiple-Instance-Groups)<br>\n",
    "**[2.2 Scheduling Strategies](#2.2-Scheduling-Strategies)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.1 Stateless Inference](#2.2.1-Stateless-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.2 Stateful Inference](#2.2.2-Stateful-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.3 Pipelines / Ensembles](#2.2.3-Pipelines-/-Ensembles)<br>\n",
    "**[2.3 Dynamic Batching](#2.3-Dynamic-Batching)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.3.1 Exercise: Implement Dynamic Batching](#2.3.1-Exercise:-Implement-Dynamic-Batching)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've executed customer requests sequentially, in the order they have arrived at the server, and used a static batch of size 8 for any requests to our server. This has not only left our GPUs heavily underutilized, but has also significantly affected the latency of responses received from the server. This is not an uncommon situation. Unless you are developing an application that processes large volumes of data in batch, you will likely be sending individual inference requests from the user application, leading to even further underutilization. As we have seen in the previous notebook, model optimizations do help considerably to accelerate model execution.  However, they do not change the fact that when serving is implemented naively, the nature of the inference workload leads to GPU underutilization.\n",
    "\n",
    "Inference servers, such as NVIDIA Triton, implement a wide range of features that allow us to improve the GPU utilization and improve request latency. The three that we will discuss in this class are:<br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/architecture.html#section-concurrent-model-execution\">Concurrent model execution</a></br>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/models_and_schedulers.html\">Scheduling</a> <br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/model_configuration.html#section-dynamic-batcher\">Dynamic batching</a> <br/>\n",
    "\n",
    "\n",
    "Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/quickstart.html\">Triton documentation</a> and its <a href=\"https://github.com/NVIDIA/triton-inference-server\">source code</a> for further information about the mechanisms and configurations that can help improve model inference performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Concurrent Model Execution\n",
    "The Triton architecture allows multiple models and/or multiple instances of the same model to execute in parallel on a single GPU. The following figure shows an example with two models: `model0` and `model1`. Assuming Triton is not currently processing any request, when two requests arrive simultaneously, one for each model, Triton immediately schedules both of them onto the GPU, and the GPUâ€™s hardware scheduler begins working on both computations in parallel. </br>\n",
    "\n",
    "<img src=\"images/multi_model_exec.png\"/><br/>\n",
    "\n",
    "#### Default Behavior\n",
    "\n",
    "By default, if multiple requests for the same model arrive at the same time, Triton will serialize their execution by scheduling only one at a time on the GPU, as shown in the following figure.\n",
    "\n",
    "<img src=\"images/multi_model_serial_exec.png\"/><br/>\n",
    "\n",
    "Triton provides an instance-group feature that allows each model to specify how many parallel executions of that model should be allowed. Each such enabled parallel execution is referred to as an *execution instance*. By default, Triton gives each model a single execution instance, which means that only a single execution of the model is allowed to be in progress at a time as shown in the above figure. \n",
    "\n",
    "#### Instance Groups\n",
    "By using the *instance-group* setting, the number of execution instances for a model can be increased. The following figure shows model execution when `model1` is configured to allow three execution instances. As shown in the figure, the first three `model1` inference requests are immediately executed in parallel on the GPU. The fourth `model1` inference request must wait until one of the first three executions completes before beginning.\n",
    "\n",
    "<img src=\"images/multi_model_parallel_exec.png\"/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Exercise: Usage Considerations\n",
    "\n",
    "For most models, the Triton feature that provides the largest performance improvement is *dynamic batching*. The key advantages of dynamic batching over setting up multiple instance execution are:\n",
    "- No overhead for model parameter storage\n",
    "- No overhead related to model parameter fetch from the GPU memory\n",
    "- Better utilization of the GPU resources\n",
    "\n",
    "Before we look at the configuration for multiple model execution, let's execute our model again using a single instance, and observe the resource utilization of the GPU. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Steps\n",
    "1. Launch a terminal window from the JupyterLab launch page.  If you need to open a new launch page, click the '+' icon on the left sidebar menu. You can then use a drag-and-drop action to move the terminal to a sub-window configuration  for better viewing.\n",
    "2. Execute the following command in the terminal before you run the performance tool:<br>\n",
    "\n",
    "```\n",
    "watch -n0.5 nvidia-smi\n",
    "```\n",
    "    You should see an output that resembles:\n",
    "<img src=\"images/NVIDIASMI.png\" style=\"position:relative; left:30px;\" width=800/>\n",
    "\n",
    "3. Execute the same benchmark we used in the previous notebook, but with the batch size reduced to 1, and observe the <code>nvidia-smi</code> output again.  Pay special attention to the memory consumption and GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previous configuration.\n",
    "modelVersion=\"1\"\n",
    "precision=\"fp32\"\n",
    "batchSize=\"1\"\n",
    "maxLatency=\"500\"\n",
    "maxClientThreads=\"10\"\n",
    "maxConcurrency=\"2\"\n",
    "dockerBridge=\"host\"\n",
    "resultsFolderName=\"1\"\n",
    "profilingData=\"utilities/profiling_data_int64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-fp16\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 41.6667 infer/sec. Avg latency: 24171 usec (std 188 usec)\n",
      "  Pass [2] throughput: 41.3333 infer/sec. Avg latency: 24192 usec (std 88 usec)\n",
      "  Pass [3] throughput: 41.3333 infer/sec. Avg latency: 24236 usec (std 65 usec)\n",
      "  Client: \n",
      "    Request count: 124\n",
      "    Throughput: 41.3333 infer/sec\n",
      "    Avg latency: 24236 usec (standard deviation 65 usec)\n",
      "    p50 latency: 24236 usec\n",
      "    p90 latency: 24316 usec\n",
      "    p95 latency: 24359 usec\n",
      "    p99 latency: 24426 usec\n",
      "    Avg HTTP time: 24230 usec (send 5 usec + response wait 24224 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 149\n",
      "    Execution count: 149\n",
      "    Successful request count: 149\n",
      "    Avg request latency: 23924 usec (overhead 4 usec + queue 30 usec + compute input 11 usec + compute infer 23868 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 41.3333 infer/sec. Avg latency: 48196 usec (std 184 usec)\n",
      "  Pass [2] throughput: 41.6667 infer/sec. Avg latency: 48225 usec (std 212 usec)\n",
      "  Pass [3] throughput: 41.3333 infer/sec. Avg latency: 48298 usec (std 221 usec)\n",
      "  Client: \n",
      "    Request count: 124\n",
      "    Throughput: 41.3333 infer/sec\n",
      "    Avg latency: 48298 usec (standard deviation 221 usec)\n",
      "    p50 latency: 48284 usec\n",
      "    p90 latency: 48541 usec\n",
      "    p95 latency: 48755 usec\n",
      "    p99 latency: 48927 usec\n",
      "    Avg HTTP time: 48312 usec (send 5 usec + response wait 48306 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 150\n",
      "    Execution count: 150\n",
      "    Successful request count: 150\n",
      "    Avg request latency: 47969 usec (overhead 3 usec + queue 23894 usec + compute input 11 usec + compute infer 24050 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 41.3333 infer/sec. Avg latency: 72630 usec (std 314 usec)\n",
      "  Pass [2] throughput: 41.3333 infer/sec. Avg latency: 72668 usec (std 287 usec)\n",
      "  Pass [3] throughput: 41.3333 infer/sec. Avg latency: 72580 usec (std 216 usec)\n",
      "  Client: \n",
      "    Request count: 124\n",
      "    Throughput: 41.3333 infer/sec\n",
      "    Avg latency: 72580 usec (standard deviation 216 usec)\n",
      "    p50 latency: 72590 usec\n",
      "    p90 latency: 72852 usec\n",
      "    p95 latency: 72913 usec\n",
      "    p99 latency: 73115 usec\n",
      "    Avg HTTP time: 72616 usec (send 5 usec + response wait 72610 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 149\n",
      "    Execution count: 149\n",
      "    Successful request count: 149\n",
      "    Avg request latency: 72244 usec (overhead 3 usec + queue 48119 usec + compute input 10 usec + compute infer 24101 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 41 infer/sec. Avg latency: 96788 usec (std 332 usec)\n",
      "  Pass [2] throughput: 41 infer/sec. Avg latency: 96850 usec (std 226 usec)\n",
      "  Pass [3] throughput: 41 infer/sec. Avg latency: 96970 usec (std 292 usec)\n",
      "  Client: \n",
      "    Request count: 123\n",
      "    Throughput: 41 infer/sec\n",
      "    Avg latency: 96970 usec (standard deviation 292 usec)\n",
      "    p50 latency: 96902 usec\n",
      "    p90 latency: 97429 usec\n",
      "    p95 latency: 97618 usec\n",
      "    p99 latency: 97741 usec\n",
      "    Avg HTTP time: 96976 usec (send 6 usec + response wait 96969 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 149\n",
      "    Execution count: 149\n",
      "    Successful request count: 149\n",
      "    Avg request latency: 96606 usec (overhead 3 usec + queue 72442 usec + compute input 12 usec + compute infer 24138 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 41.3333 infer/sec. Avg latency: 121771 usec (std 1100 usec)\n",
      "  Pass [2] throughput: 41 infer/sec. Avg latency: 121213 usec (std 381 usec)\n",
      "  Pass [3] throughput: 41.3333 infer/sec. Avg latency: 121316 usec (std 391 usec)\n",
      "  Client: \n",
      "    Request count: 124\n",
      "    Throughput: 41.3333 infer/sec\n",
      "    Avg latency: 121316 usec (standard deviation 391 usec)\n",
      "    p50 latency: 121194 usec\n",
      "    p90 latency: 121880 usec\n",
      "    p95 latency: 122007 usec\n",
      "    p99 latency: 122426 usec\n",
      "    Avg HTTP time: 121279 usec (send 5 usec + response wait 121273 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 149\n",
      "    Execution count: 149\n",
      "    Successful request count: 149\n",
      "    Avg request latency: 120908 usec (overhead 3 usec + queue 96732 usec + compute input 10 usec + compute infer 24152 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 41 infer/sec. Avg latency: 145729 usec (std 446 usec)\n",
      "  Pass [2] throughput: 41.3333 infer/sec. Avg latency: 146042 usec (std 471 usec)\n",
      "  Pass [3] throughput: 41.3333 infer/sec. Avg latency: 146119 usec (std 324 usec)\n",
      "  Client: \n",
      "    Request count: 124\n",
      "    Throughput: 41.3333 infer/sec\n",
      "    Avg latency: 146119 usec (standard deviation 324 usec)\n",
      "    p50 latency: 146150 usec\n",
      "    p90 latency: 146515 usec\n",
      "    p95 latency: 146618 usec\n",
      "    p99 latency: 146941 usec\n",
      "    Avg HTTP time: 146073 usec (send 6 usec + response wait 146066 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 148\n",
      "    Execution count: 148\n",
      "    Successful request count: 148\n",
      "    Avg request latency: 145691 usec (overhead 4 usec + queue 121429 usec + compute input 11 usec + compute infer 24235 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 41 infer/sec. Avg latency: 170594 usec (std 346 usec)\n",
      "  Pass [2] throughput: