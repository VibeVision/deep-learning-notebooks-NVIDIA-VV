{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Transformer Encoder\n",
    "\n",
    "In this notebook, you'll look deeper at the encoder mechanics, positional encoding, and how self-attention works.\n",
    "\n",
    "**[2.1 Overview](#2.1-Overview)<br>**\n",
    "**[2.2 Embedding](#2.2-Embedding)<br>**\n",
    "**[2.3 Positional Encoding](#2.3-Positional-Encoding)<br>**\n",
    "**[2.4 Self-Attention](#2.4-Self-Attention)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.4.1 Self-Attention Matrix Calculation](#2.4.1-Self-Attention-Matrix-Calculation)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.4.2 Visualization of Attention](#2.4.2-Visualization-of-Attention)<br>\n",
    "**[2.5 Multi-Head Attention](#2.5-Multi-Head-Attention)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Overview\n",
    "In the Transformer paper, both encoder and decoder are composed of $\\mathbf{N = 6}$ identical layers each, for a total of 12 layers. Each of the six encoder layers has two sub-layers: The first is a multi-head self-attention mechanism; the second is a simple, position-wise fully connected feed-forward network.  \n",
    "\n",
    "The encoder’s purpose is to encode a source sentence into hidden state vectors; the decoder uses the last representation of the state vectors to predict characters in the target language. \n",
    "\n",
    "Let's see what is happening inside an encode block. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/encoder1.png\" width=\"500\"></center>\n",
    "\n",
    "<center> Figure 4. Encoder Block illustration. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder code is provided below. A number of `TransformerEncoderLayers` (default 6) are added to `self.layers`, which will be called when we forward pass through the model.\n",
    "\n",
    "When `TransformerEncoder.forward()` is called, the input source tokens (`src_tokens`) are embedded (using `embed_tokens`) and then added to the positional encodings (see the description later). After dropout and some tensor manipulation, our embedded tokens `x` are passed through each of the six `TranformerEncoderLayers`, in the loop that starts `for layer in self.layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import seaborn as sns\n",
    "\n",
    "from encoder_demos.demo_fairseq.models.fairseq_model import BaseFairseqModel, FairseqDecoder, FairseqEncoder\n",
    "from encoder_demos.demo_fairseq.models.fairseq_incremental_decoder import FairseqIncrementalDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, args, embed_tokens, left_pad=True):\n",
    "        super().__init__()\n",
    "        self.dropout = args.dropout\n",
    "        self.fuse_dropout_add = args.fuse_dropout_add\n",
    "        self.fuse_relu_dropout = args.fuse_relu_dropout\n",
    "\n",
    "        embed_dim = embed_tokens.embedding_dim\n",
    "        self.padding_idx = embed_tokens.padding_idx\n",
    "        self.max_source_positions = args.max_source_positions\n",
    "\n",
    "        self.embed_tokens = embed_tokens\n",
    "        self.embed_scale = math.sqrt(embed_dim)\n",
    "        self.embed_positions = PositionalEmbedding(\n",
    "            args.max_source_positions, embed_dim, self.padding_idx,\n",
    "            left_pad=left_pad,\n",
    "            learned=args.encoder_learned_pos,\n",
    "        ) if not args.no_token_positional_embeddings else None\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.layers.extend([\n",
    "            TransformerEncoderLayer(args)\n",
    "            for i in range(args.encoder_layers)\n",
    "        ])\n",
    "\n",
    "        self.normalize = args.encoder_normalize_before\n",
    "        if self.normalize:\n",
    "            self.layer_norm = FusedLayerNorm(embed_dim) if args.fuse_layer_norm else nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_lengths):\n",
    "        # embed tokens and positions\n",
    "        x = self.embed_scale * self.embed_tokens(src_tokens)\n",
    "        if self.embed_positions is not None:\n",
    "            x += self.embed_positions(src_tokens)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        # The tensor needs to copy transposed because\n",
    "        # fused dropout is not capable of handing strided data\n",
    "        if self.fuse_dropout_add :\n",
    "            x = x.transpose(0, 1).contiguous()\n",
    "        else :\n",
    "            x = x.transpose(0, 1)\n",
    "\n",
    "        # compute padding mask\n",
    "        encoder_padding_mask = src_tokens.eq(self.padding_idx)\n",
    "        if not encoder_padding_mask.any():\n",
    "            _encoder_padding_mask = None\n",
    "        else:\n",
    "            _encoder_padding_mask = encoder_padding_mask\n",
    "\n",
    "        # encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, _encoder_padding_mask)\n",
    "\n",
    "        if self.normalize:\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "        return x, encoder_padding_mask # x.shape == T x B x C, encoder_padding_mask.shape == B x T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `TransformerEncoderLayer` is a copy of the TransformerEncoderLayer class defined [here](https://github.com/NVIDIA/DeepLearningExamples/blob/8c3514071275b2805b29372f6dabe515d431416f/PyTorch/Translation/Transformer/fairseq/models/transformer.py#L420). The full implementation includes optional layer normalization (removed here for the sake of simplicity).\n",
    "\n",
    "Looking at the `forward` method, the embedded tokens `x` are passed through the self attention mechanism (explained below). By default `self.fuse_dropout_add` is `True` and `self.fuse_relu_dropout` is `False`, so the results of the self attention pass through a linear layer `fc1`, then have dropout applied, then through the second linear layer `fc2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand how an encoder block works, we need the concept of *embedding*. We begin by turning each input word into a vector using an embedding algorithm. Word embedding is really all about improving the ability of networks to learn from text data. In simple terms, word embeddings are vector representations of a particular word. For more detailed information on embedding algorithms please see [Word2Vec](https://arxiv.org/pdf/1310.4546.pdf), [GloVe](https://nlp.stanford.edu/projects/glove/), and [fastText](https://fasttext.cc/).\n",
    "\n",
    "Note that the embedding only happens in the bottom-most encoder. In Figure 4, the two input words are represented vectors. These are the embedding vectors. The Transformer model uses $\\mathbf{d_{model} = 512}$, however this lab, and the NVIDIA implementation, use the “transformer-big” model $\\mathbf{d_{model} = 1024}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to tokenize some input text. Tokenization converts a sentence into a list of numbers, ending in a 2, the representation for \"end of sentence (EOS)\". The tokenized representation will be combined with the positional encoder, to create the embedded vector, which is the input to the self-attention layer shown in Figure 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| [en] dictionary: 33712 types\n",
      "| [de] dictionary: 33712 types\n",
      "\n",
      "Input text:        I am looking for a place to eat.\n",
      "Tokenized output:  [  40  105 1804   19   14  358   12 9637    5    2]\n"
     ]
    }
   ],
   "source": [
    "import encoder_demos.tokenize as tok\n",
    "\n",
    "input_text = \"I am looking for a place to eat.\"\n",
    "tokens = tok.demo(input_text)\n",
    "print(\"\\nInput text:        %s\\nTokenized output: \" % input_text, tokens[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now change the input text and see how the tokenized output vector is altered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| [en] dictionary: 33712 types\n",
      "| [de] dictionary: 33712 types\n",
      "| loading model(s) from /data/checkpoints/JoC_Transformer_FP32_PyT_20190304.pt\n",
      "| Sentences are being padded to multiples of: 1\n",
      "generated batches in  0.0002791881561279297 s\n",
      "En: I am looking for a place to eat.\n",
      "German: Ich bin auf der Suche nach einem Ort zu ße.\n",
      "\n",
      "H: -0.3850874900817871\n",
      "H is the hypothesis along with an average log-likelihood\n"
     ]
    }
   ],
   "source": [
    "# Translate an input sentence in English to German\n",
    "\n",
    "import encoder_demos.functional_translation as ft\n",
    "input_sentence = \"I am looking for a place to eat.\"\n",
    "\n",
    "e, g, h = ft.demo(input_sentence)\n",
    "print(\"En:\", e)\n",
    "print(\"German:\", g)\n",
    "print('')\n",
    "print(\"H:\", h)\n",
    "print('H is the hypothesis along with an average log-likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis scores are output as log-probabilities, thus are negative. We can calculate the probability of this hypothesis as exp(H)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of H = 0.6804\n"
     ]
    }
   ],
   "source": [
    "print('Probability of H = {}'.format(round(np.exp(h),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Positional Encoding \n",
    "\n",
    "Language models need to make use of the sequential nature of words in a sentence. Since the Transformer model contains no recurrent or convolutional units, positional encodings (PEs) are used to account for the order of the words in the input sequence. The positional encodings have the same dimension, d<sub>model</sub>, as the embeddings, so that the two can be summed (see Figure 4). This allows the model to understand the position of each word in the input text.\n",
    "\n",
    "In the paper, the authors use sine and cosine functions of different frequencies for positional encoding:\n",
    "\n",
    "\n",
    "<img src=\"images/pe.png\" width=\"400\">\n",
    "where <i>pos</i> is the position and <i>i</i> is the dimension. Let's explain the formula above via an example:\n",
    "\n",
    "Let's assume that  d<sub>model</sub> = 4. This means that word 𝑤 at input sequence position <i>pos</i> ∈ [0, 𝐿−1] is represented with a 4-dimensional embedding 𝑒<sub>𝑤</sub> vector. Setting <i>i</i> ∈ [0, 255], then, for even indices  of 4-dimensional embedding vector, we will use sin(pos/10000<i><sup>2i/d<sub>model</