{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Transformer Encoder\n",
    "\n",
    "In this notebook, you'll look deeper at the encoder mechanics, positional encoding, and how self-attention works.\n",
    "\n",
    "**[2.1 Overview](#2.1-Overview)<br>**\n",
    "**[2.2 Embedding](#2.2-Embedding)<br>**\n",
    "**[2.3 Positional Encoding](#2.3-Positional-Encoding)<br>**\n",
    "**[2.4 Self-Attention](#2.4-Self-Attention)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.4.1 Self-Attention Matrix Calculation](#2.4.1-Self-Attention-Matrix-Calculation)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.4.2 Visualization of Attention](#2.4.2-Visualization-of-Attention)<br>\n",
    "**[2.5 Multi-Head Attention](#2.5-Multi-Head-Attention)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Overview\n",
    "In the Transformer paper, both encoder and decoder are composed of $\\mathbf{N = 6}$ identical layers each, for a total of 12 layers. Each of the six encoder layers has two sub-layers: The first is a multi-head self-attention mechanism; the second is a simple, position-wise fully connected feed-forward network.  \n",
    "\n",
    "The encoderâ€™s purpose is to encode a source sentence into hidden state vectors; the decoder uses the last representation of the state vectors to predict characters in the target language. \n",
    "\n",
    "Let's see what is happening inside an encode block. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/encoder1.png\" width=\"500\"></center>\n",
    "\n",
    "<center> Figure 4. Encoder Block illustration. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder code is provided below. A number of `TransformerEncoderLayers` (default 6) are added to `self.layers`, which will be called when we forward pass through the model.\n",
    "\n",
    "When `TransformerEncoder.forward()` is called, the input source tokens (`src_tokens`) are embedded (using `embed_tokens`) and then added to the positional encodings (see the description later). After dropout and some tensor manipulation, our embedded tokens `x` are passed through each of the six `TranformerEncoderLayers`, in the loop that starts `for layer in self.layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import seaborn as sns\n",
    "\n",
    "from encoder_demos.demo_fairseq.models.fairseq_model import BaseFairseqModel, FairseqDecoder, FairseqEncoder\n",
    "from encoder_demos.demo_fairseq.models.fairseq_incremental_decoder import FairseqIncrementalDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, args, embed_tokens, left_pad=True):\n",
    "        super().__init__()\n",
    "        self.dropout = args.dropout\n",
    "        self.fuse_dropout_add = args.fuse_dropout_add\n",
    "        self.fuse_relu_dropout = args.fuse_relu_dropout\n",
    "\n",
    "        embed_dim = embed_tokens.embedding_dim\n",
    "        self.padding_idx = embed_tokens.padding_idx\n",
    "        self.max_source_positions = args.max_source_positions\n",
    "\n",
    "        self.embed_tokens = embed_tokens\n",
    "        self.embed_scale = math.sqrt(embed_dim)\n",
    "        self.embed_positions = PositionalEmbedding(\n",
    "            args.max_source_positions, embed_dim, self.padding_idx,\n",
    "            left_pad=left_pad,\n",
    "            learned=args.encoder_learned_pos,\n",
    "        ) if not args.no_token_positional_embeddings else None\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.layers.extend([\n",
    "            TransformerEncoderLayer(args)\n",
    "            for i in range(args.encoder_layers)\n",
    "        ])\n",
    "\n",
    "        self.normalize = args.encoder_normalize_before\n",
    "        if self.normalize:\n",
    "            self.layer_norm = FusedLayerNorm(embed_dim) if args.fuse_layer_norm else nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_lengths):\n",
    "        # embed 