{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Your Model\n",
    "Now that we have a well trained model, it's time to use it. In this exercise, we'll expose new images to our model and detect the correct letters of the sign language alphabet. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load an already-trained model from disk\n",
    "* Reformat images for a model trained on images of a different format\n",
    "* Perform inference with new images, never seen by the trained model and evaluate its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model\n",
    "Now that we're in a new notebook, let's load the saved model that we trained. Our save from the previous exercise created a folder called \"asl_model\". We can load the model by selecting the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.load_model('asl_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to make sure everything looks intact, you can see the summary of the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 75)        750       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 75)        300       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 75)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 50)        33800     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 50)        200       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 25)          11275     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 7, 25)          100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 25)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               205312    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                12312     \n",
      "=================================================================\n",
      "Total params: 264,049\n",
      "Trainable params: 263,749\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing an Image for the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to use the model to make predictions on new images that it's never seen before. This is also called inference. We've given you a set of images in the asl_images folder. Try opening it using the left navigation and explore the images.\n",
    "\n",
    "You'll notice that the images we have are much higher resolution than the images in our dataset. They are also in color. Remember that our images in the dataset were 28x28 pixels and grayscale. It's important to keep in mind that whenever you make predictions with a model, the input must match the shape of the data that the model was trained on. For this model, the training dataset was of the shape: (27455, 28, 28, 1). This corresponded to 27455 images of 28 by 28 pixels each with one color channel (grayscale). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use our model to make predictions on new images, it will be useful to show the image as well. We can use the matplotlib library to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def show_image(image_path):\n",
    "    image = mpimg.imread(image_path)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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